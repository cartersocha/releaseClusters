{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Read In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "filepath1 = r'/Users/cartersocha/Downloads/instgramHashtagCounts.xlsx'\n",
    "instaHashtagDf = pd.read_excel(filepath1)\n",
    "\n",
    "filepath2 = r'/Users/cartersocha/Downloads/tweetCountTest.xlsx'\n",
    "dailyTweetDf = pd.read_excel(filepath2)\n",
    "\n",
    "filepath3 = r'/Users/cartersocha/Desktop/ReleaseData.xlsx'\n",
    "releaseDf = pd.read_excel(filepath3, \"ShowInfoEndStart\")\n",
    "\n",
    "filepath4 = r'/Users/cartersocha/Downloads/instgramAccountCounts.xlsx'\n",
    "igAccountDf = pd.read_excel(filepath4)\n",
    "\n",
    "filepath5 = r'/Users/cartersocha/Downloads/redditCountTest.xlsx'\n",
    "redditSubsDf = pd.read_excel(filepath5)\n",
    "\n",
    "filepath6 = r'/Users/cartersocha/Downloads/redditCommentCombo.csv'\n",
    "redditCommentsDf = pd.read_csv(filepath6)\n",
    "\n",
    "filepath8 = r'/Users/cartersocha/Desktop/ReleaseData.xlsx'\n",
    "releaseDateDf = pd.read_excel(filepath8, \"ReleaseDateData\")\n",
    "\n",
    "filepath10 = r'/Users/cartersocha/Downloads/googleDataset2.csv'\n",
    "googleDailyData = pd.read_csv(filepath10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DateTimeConvert(dateDf, dateColumn):\n",
    "    dateDf[dateColumn] = pd.to_datetime(dateDf[dateColumn])  \n",
    "\n",
    "    return dateDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiffMaker(fillnaDf, valueColumn, dateColumn):\n",
    "    fillnaDf.sort_values(['TvShow', dateColumn], inplace=True)\n",
    "\n",
    "    fillnaDf['diffs'] = fillnaDf.groupby(['TvShow'])[valueColumn].transform(lambda x: x.diff()).fillna(0)\n",
    "\n",
    "    fillnaDf.sort_index(inplace=True)\n",
    "\n",
    "    return fillnaDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveData(release,showDf):\n",
    "\n",
    "    bigDf = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(release)):\n",
    "        show = release['TvShow'][i]\n",
    "        firstDate = release['Release Date'][i]\n",
    "        secondDate = release['90DayDate'][i]\n",
    "\n",
    "        smallDf = showDf[showDf['TvShow'] == show]\n",
    "\n",
    "        newdf = smallDf[smallDf['RunDate'].between(firstDate, secondDate)]\n",
    "\n",
    "        bigDf = bigDf.append(newdf,ignore_index=True)\n",
    "\n",
    "    return bigDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MergeDfs(mainDf, secondDf, columnName):\n",
    "\n",
    "    merged = pd.merge(mainDf,secondDf, how='outer', on=columnName)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def NegativeDiffs(diffDf,columnName):\n",
    "\n",
    "    diffDf['zeroedDiffs'] = np.where((diffDf[columnName] < 0), 0, diffDf[columnName])\n",
    "\n",
    "    return diffDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SumSocialMedia(sumDf,columnName,trigger):\n",
    "    # check to see if this is episode count or generic summarization\n",
    "    if trigger == 1:\n",
    "        summarizedDf = sumDf.groupby(columnName, as_index=False).sum()\n",
    "        summarizedDf = pd.DataFrame(summarizedDf)\n",
    "    \n",
    "    else:\n",
    "        summarizedDf = sumDf.groupby(columnName, as_index=False).count()\n",
    "        summarizedDf = pd.DataFrame(summarizedDf)\n",
    "    \n",
    "    return summarizedDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import distribution\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def NormalizeData(normalDf,columnName):\n",
    "\n",
    "    # define min max scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler1 = StandardScaler()\n",
    "    scaler2 = MaxAbsScaler()\n",
    "    scaler3 = RobustScaler()\n",
    "    scaler4 = Normalizer()\n",
    "    scaler5 = QuantileTransformer(output_distribution='normal')\n",
    "    scaler6 = PowerTransformer()\n",
    "\n",
    "    # transform data\n",
    "    normalDf['ScaledDataAdjust-MinMax'] = scaler.fit_transform(normalDf[[columnName]])\n",
    "    normalDf['ScaledDataAdjust-StandardScaler'] = scaler1.fit_transform(normalDf[[columnName]])\n",
    "    normalDf['ScaledDataAdjust-MaxAbsScaler'] = scaler2.fit_transform(normalDf[[columnName]])   \n",
    "    normalDf['ScaledDataAdjust-RobustScaler'] = scaler3.fit_transform(normalDf[[columnName]])   \n",
    "    normalDf['ScaledDataAdjust-Normalizer'] = scaler4.fit_transform(normalDf[[columnName]])   \n",
    "    normalDf['ScaledDataAdjust-QuantileTransformer'] = scaler5.fit_transform(normalDf[[columnName]])\n",
    "    normalDf['ScaledDataAdjust-PowerTransformer'] = scaler6.fit_transform(normalDf[[columnName]])       \n",
    "    \n",
    "    return normalDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Release Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "releaseDf['90DayDate'] = releaseDf['Release Date'] + pd.DateOffset(days=120)\n",
    "\n",
    "releaseDf = DateTimeConvert(releaseDf,'90DayDate')\n",
    "releaseDf = DateTimeConvert(releaseDf,'Release Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instagram Hashtag Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/2b53j97121jg00pcthbt6kz80000gn/T/ipykernel_69192/2781502079.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bigDf = bigDf.append(newdf,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "instaHashtagDf = DateTimeConvert(instaHashtagDf,'RunDate')\n",
    "\n",
    "instaHashtagDf = DiffMaker(instaHashtagDf,'HashtagValue','RunDate')\n",
    "\n",
    "instaHashDf = RemoveData(releaseDf, instaHashtagDf)\n",
    "instaHashDf = NegativeDiffs(instaHashDf, 'diffs')\n",
    "instaHashDf['SocialMediaSource'] = 'InstagramHashtag'\n",
    "instaHashDf['SocialMediaValue'] = instaHashDf['zeroedDiffs']\n",
    "\n",
    "instaHashDf = NormalizeData(instaHashDf,'SocialMediaValue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/2b53j97121jg00pcthbt6kz80000gn/T/ipykernel_69192/2781502079.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bigDf = bigDf.append(newdf,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "dailyTweetDf = DateTimeConvert(dailyTweetDf,'RunDate')\n",
    "dailyTweetDf = RemoveData(releaseDf, dailyTweetDf)\n",
    "dailyTweetDf['SocialMediaValue'] = dailyTweetDf['TweetCount'] + dailyTweetDf['RetweetCount']\n",
    "dailyTweetDf['SocialMediaSource'] = 'Tweets'\n",
    "dailyTweetDf = NormalizeData(dailyTweetDf,'SocialMediaValue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reddit Comment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/2b53j97121jg00pcthbt6kz80000gn/T/ipykernel_69192/2781502079.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bigDf = bigDf.append(newdf,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "redditCommentsDf = DateTimeConvert(redditCommentsDf,'RunDate')\n",
    "redCommentDf = RemoveData(releaseDf, redditCommentsDf)\n",
    "redCommentDf['SocialMediaValue'] = redCommentDf['NumComments'] + redCommentDf['score']\n",
    "\n",
    "redCommentDf = redCommentDf[['TvShow','RunDate','SocialMediaValue']]\n",
    "iRedCommentDf = SumSocialMedia(redCommentDf, ['TvShow','RunDate'],1)\n",
    "iRedCommentDf['SocialMediaSource'] = 'RedditComments'\n",
    "\n",
    "iRedCommentDf = NormalizeData(iRedCommentDf,'SocialMediaValue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Instagram Account Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/2b53j97121jg00pcthbt6kz80000gn/T/ipykernel_69192/2781502079.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bigDf = bigDf.append(newdf,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "igAccountDf = DateTimeConvert(igAccountDf,'RunDate')\n",
    "\n",
    "igAccountDf = DiffMaker(igAccountDf,'IgAccountCounts','RunDate')\n",
    "\n",
    "\n",
    "instaAccDf = RemoveData(releaseDf, igAccountDf)\n",
    "instaAccDf = NegativeDiffs(instaAccDf, 'diffs')\n",
    "instaAccDf['SocialMediaSource'] = 'InstagramAccount'\n",
    "instaAccDf['SocialMediaValue'] = instaAccDf['zeroedDiffs']\n",
    "\n",
    "instaAccDf = NormalizeData(instaAccDf,'SocialMediaValue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reddit Subscriber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/2b53j97121jg00pcthbt6kz80000gn/T/ipykernel_69192/2781502079.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bigDf = bigDf.append(newdf,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "redditSubsDf = DateTimeConvert(redditSubsDf,'RunDate')\n",
    "\n",
    "redditSubsDf = DiffMaker(redditSubsDf,'RedditSubs','RunDate')\n",
    "\n",
    "redSubDf = RemoveData(releaseDf, redditSubsDf)\n",
    "redSubDf = NegativeDiffs(redSubDf, 'diffs')\n",
    "redSubDf['SocialMediaSource'] = 'RedditSubscribers'\n",
    "redSubDf['SocialMediaValue'] = redSubDf['zeroedDiffs']\n",
    "\n",
    "redSubDf = NormalizeData(redSubDf,'SocialMediaValue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Google Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7v/2b53j97121jg00pcthbt6kz80000gn/T/ipykernel_69192/2781502079.py:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  bigDf = bigDf.append(newdf,ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "googleDailyData = DateTimeConvert(googleDailyData,'RunDate')\n",
    "googDf1 = RemoveData(releaseDf, googleDailyData)\n",
    "googDf1 = NegativeDiffs(googDf1, 'GoogleValue')\n",
    "\n",
    "googDf1['SocialMediaSource'] = 'Google'\n",
    "googDf1['SocialMediaValue'] = googDf1['zeroedDiffs']\n",
    "\n",
    "googDf1 = NormalizeData(googDf1,'SocialMediaValue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Release Date Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "releaseDateDf = DateTimeConvert(releaseDateDf,'EpisodeReleaseDate')\n",
    "iReleaseData = SumSocialMedia(releaseDateDf, ['TvShow','EpisodeReleaseDate'],0)\n",
    "iReleaseData['DailyReleaseCount'] = iReleaseData['EpisodeNumber']\n",
    "iReleaseData['RunDate'] = iReleaseData['EpisodeReleaseDate']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Join the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Time Value Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "instaAccDf = instaAccDf[['TvShow','RunDate','SocialMediaValue','SocialMediaSource', 'ScaledDataAdjust-QuantileTransformer']]#,'ScaledDataAdjust-MinMax','ScaledDataAdjust-StandardScaler',\n",
    "#'ScaledDataAdjust-MaxAbsScaler','ScaledDataAdjust-RobustScaler','ScaledDataAdjust-Normalizer','ScaledDataAdjust-QuantileTransformer'\n",
    "#,'ScaledDataAdjust-PowerTransformer']]#,'diffs']]\n",
    "redSubDf = redSubDf[['TvShow','RunDate','SocialMediaValue','SocialMediaSource', 'ScaledDataAdjust-QuantileTransformer']]#,'ScaledDataAdjust-MinMax','ScaledDataAdjust-StandardScaler',\n",
    "#'ScaledDataAdjust-MaxAbsScaler','ScaledDataAdjust-RobustScaler','ScaledDataAdjust-Normalizer','ScaledDataAdjust-QuantileTransformer'\n",
    "#,'ScaledDataAdjust-PowerTransformer']]#,'diffs']]\n",
    "dailyTweetDf = dailyTweetDf[['TvShow','RunDate','SocialMediaValue','SocialMediaSource', 'ScaledDataAdjust-QuantileTransformer']]#,'ScaledDataAdjust-MinMax','ScaledDataAdjust-StandardScaler',\n",
    "#'ScaledDataAdjust-MaxAbsScaler','ScaledDataAdjust-RobustScaler','ScaledDataAdjust-Normalizer','ScaledDataAdjust-QuantileTransformer'\n",
    "#,'ScaledDataAdjust-PowerTransformer']]#,'diffs']]\n",
    "instaHashDf = instaHashDf[['TvShow','RunDate','SocialMediaValue','SocialMediaSource', 'ScaledDataAdjust-QuantileTransformer']]#'ScaledDataAdjust-MinMax','ScaledDataAdjust-StandardScaler',\n",
    "#'ScaledDataAdjust-MaxAbsScaler','ScaledDataAdjust-RobustScaler','ScaledDataAdjust-Normalizer','ScaledDataAdjust-QuantileTransformer'\n",
    "#,'ScaledDataAdjust-PowerTransformer']]#,'diffs']]\n",
    "googDf1 = googDf1[['TvShow','RunDate','SocialMediaValue','SocialMediaSource', 'ScaledDataAdjust-QuantileTransformer']]#,'ScaledDataAdjust-MinMax','ScaledDataAdjust-StandardScaler',\n",
    "#'ScaledDataAdjust-MaxAbsScaler','ScaledDataAdjust-RobustScaler','ScaledDataAdjust-Normalizer','ScaledDataAdjust-QuantileTransformer'\n",
    "#,'ScaledDataAdjust-PowerTransformer']]#,'diffs']]\n",
    "# reddit comment data is combined elsewhere\n",
    "iReleaseData = iReleaseData[['TvShow','RunDate','DailyReleaseCount']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "uberDf2 = pd.concat([instaAccDf,redSubDf,dailyTweetDf,instaHashDf,googDf1,iRedCommentDf])\n",
    "\n",
    "uberDf3 = MergeDfs(uberDf2, releaseDf,'TvShow')\n",
    "uberDf3\n",
    "\n",
    "uberDf3 = MergeDfs(uberDf3, iReleaseData,['TvShow','RunDate'])\n",
    "uberDf3 = uberDf3[uberDf3['Ignore'] == 1]\n",
    "uberDf3 = uberDf3.fillna(0)\n",
    "uberDf3['PostReleaseDay'] = (uberDf3['RunDate'] - uberDf3['Release Date']).astype(str).str.replace(' days','').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output - individuals & uber dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "uberDf3 = uberDf3.sort_values(['TvShow','RunDate','SocialMediaSource'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "uberdf4= uberDf3[uberDf3['PostReleaseDay'] <= 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath2 = r'/Users/cartersocha/Downloads/uberDataset7.csv'\n",
    "\n",
    "uberdf4.to_csv(filepath2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_tweets_by_show = uberdf4.groupby(['Stream','TvShow','SocialMediaSource']).max('PostReleaseDay')['PostReleaseDay']\n",
    "max_tweets_df = pd.DataFrame(max_tweets_by_show).reset_index().sort_values('PostReleaseDay', ascending=False)\n",
    "maxdfff = pd.DataFrame(max_tweets_df.groupby(['TvShow','SocialMediaSource']).max('PostReleaseDay')).reset_index()\n",
    "\n",
    "maxdfff[maxdfff['PostReleaseDay'] < 70]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
